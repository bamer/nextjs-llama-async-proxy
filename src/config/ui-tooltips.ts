import { TooltipContent } from "./tooltip-config.types";

export const gpuTooltips: Record<string, TooltipContent> = {
  device: {
    title: "Device",
    description: "Device to use for computation.",
    recommendedValue: "e.g., cuda, metal, cpu",
    effectOnModel: "Selects compute backend. CUDA for NVIDIA, Metal for Apple Silicon.",
    whenToAdjust: "Set based on your hardware. Auto-detection usually works.",
  },
  list_devices: {
    title: "List Devices",
    description: "Whether to list available devices. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "When enabled, logs available devices.",
    whenToAdjust: "Use for debugging device selection.",
  },
  gpu_layers: {
    title: "GPU Layers",
    description: "Number of model layers to offload to GPU. -1 offloads all possible layers.",
    recommendedValue: "-1 or 0-n (default: -1)",
    effectOnModel: "More layers = faster inference but more VRAM used.",
    whenToAdjust: "Decrease if you run out of VRAM. Increase for maximum speed.",
  },
  split_mode: {
    title: "Split Mode",
    description: "How to split model across multiple GPUs.",
    recommendedValue: "empty, layer, or row",
    effectOnModel: "Layer splits by layers, row splits tensors within layers.",
    whenToAdjust: "Configure for multi-GPU setups to optimize VRAM usage.",
  },
  tensor_split: {
    title: "Tensor Split",
    description: "Comma-separated list of VRAM allocations for each GPU.",
    recommendedValue: "e.g., 8,8,8 for 8GB each on 3 GPUs",
    effectOnModel: "Controls VRAM allocation per GPU in multi-GPU setups.",
    whenToAdjust: "Set based on each GPU's VRAM capacity.",
  },
  main_gpu: {
    title: "Main GPU",
    description: "Primary GPU for main model operations.",
    recommendedValue: "GPU index (default: 0)",
    effectOnModel: "Specifies which GPU handles the main computation.",
    whenToAdjust: "Set to fastest GPU in heterogeneous multi-GPU setups.",
  },
  kv_offload: {
    title: "KV Offload",
    description: "Whether to offload KV cache to GPU. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Offloading KV cache to GPU improves speed but uses VRAM.",
    whenToAdjust: "Enable if you have spare VRAM for faster inference.",
  },
  repack: {
    title: "Repack",
    description: "Whether to repack tensors for better GPU utilization. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Repacking can improve performance but adds startup time.",
    whenToAdjust: "Enable if startup time isn't critical and you want max performance.",
  },
  no_host: {
    title: "No Host",
    description: "Whether to disable host memory usage. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "When enabled, forces all computation on device.",
    whenToAdjust: "Use only if device has enough memory for entire model.",
  },
};

export const advancedTooltips: Record<string, TooltipContent> = {
  swa_full: {
    title: "SWA Full",
    description: "Sliding Window Attention full mode. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Enables full sliding window attention for better context handling.",
    whenToAdjust: "Enable for models that support SWA for improved performance.",
  },
  override_tensor: {
    title: "Override Tensor",
    description: "Override specific tensor settings.",
    recommendedValue: "Tensor specification",
    effectOnModel: "Manually override tensor configuration.",
    whenToAdjust: "For advanced debugging or optimization.",
  },
  cpu_moe: {
    title: "CPU MoE",
    description: "Whether to handle Mixture of Experts on CPU. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Forces MoE computation on CPU instead of GPU.",
    whenToAdjust: "Use when GPU memory is limited or for MoE models.",
  },
  n_cpu_moe: {
    title: "N CPU MoE",
    description: "Number of MoE experts to handle on CPU.",
    recommendedValue: "0 - 64 (default: 0)",
    effectOnModel: "Limits number of experts processed on CPU.",
    whenToAdjust: "Adjust based on CPU capabilities.",
  },
  kv_unified: {
    title: "KV Unified",
    description: "Unified KV cache mode. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Uses unified cache for keys and values.",
    whenToAdjust: "Enable if supported by model for memory savings.",
  },
  pooling: {
    title: "Pooling",
    description: "Pooling layer type for embedding models.",
    recommendedValue: "e.g., mean, cls, last",
    effectOnModel: "Determines how tokens are pooled into embeddings.",
    whenToAdjust: "Set based on model architecture and use case.",
  },
  context_shift: {
    title: "Context Shift",
    description: "Enables context shift for very long sequences. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Allows handling sequences longer than model's context window.",
    whenToAdjust: "Enable when working with very long inputs.",
  },
  rpc: {
    title: "RPC",
    description: "RPC server configuration for distributed inference.",
    recommendedValue: "RPC configuration string",
    effectOnModel: "Enables distributed inference across machines.",
    whenToAdjust: "Configure for distributed setups.",
  },
  offline: {
    title: "Offline",
    description: "Offline mode. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Disables certain online features or checks.",
    whenToAdjust: "Use in air-gapped environments or when network access is restricted.",
  },
  override_kv: {
    title: "Override KV",
    description: "Override KV cache settings.",
    recommendedValue: "KV cache specification",
    effectOnModel: "Manually configure KV cache behavior.",
    whenToAdjust: "For advanced memory optimization.",
  },
  op_offload: {
    title: "Op Offload",
    description: "Operation offload configuration. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Offloads specific operations to device.",
    whenToAdjust: "Enable for operations that benefit from GPU acceleration.",
  },
  fit: {
    title: "Fit",
    description: "Model fit strategy.",
    recommendedValue: "e.g., context, speed",
    effectOnModel: "Controls how model fits in available memory.",
    whenToAdjust: "Set based on whether you prioritize context length or speed.",
  },
  fit_target: {
    title: "Fit Target",
    description: "Target for model fitting.",
    recommendedValue: "0 - 100 (default: 0)",
    effectOnModel: "Sets optimization target for fitting.",
    whenToAdjust: "Adjust based on memory availability.",
  },
  fit_ctx: {
    title: "Fit Context",
    description: "Context size target for model fitting.",
    recommendedValue: "0 - 32768 (default: 0)",
    effectOnModel: "Sets target context length when fitting model.",
    whenToAdjust: "Set to desired context length.",
  },
  check_tensors: {
    title: "Check Tensors",
    description: "Whether to validate tensors. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Performs tensor validation checks.",
    whenToAdjust: "Enable for debugging model loading issues.",
  },
  sleep_idle_seconds: {
    title: "Sleep Idle Seconds",
    description: "Seconds to sleep when idle before resource cleanup.",
    recommendedValue: "0 - 3600 (default: 0)",
    effectOnModel: "Controls idle timeout for resource management.",
    whenToAdjust: "Set to free resources after inactivity.",
  },
  polling: {
    title: "Polling",
    description: "Polling configuration.",
    recommendedValue: "Polling configuration string",
    effectOnModel: "Controls polling behavior for async operations.",
    whenToAdjust: "Configure based on async handling requirements.",
  },
  polling_batch: {
    title: "Polling Batch",
    description: "Batch polling configuration.",
    recommendedValue: "Batch polling string",
    effectOnModel: "Controls how batches are polled.",
    whenToAdjust: "Configure for optimal batch processing.",
  },
  reasoning_format: {
    title: "Reasoning Format",
    description: "Format for reasoning outputs.",
    recommendedValue: "e.g., think, chain",
    effectOnModel: "Controls how reasoning is formatted in output.",
    whenToAdjust: "Set based on desired reasoning output format.",
  },
  reasoning_budget: {
    title: "Reasoning Budget",
    description: "Token budget for reasoning.",
    recommendedValue: "0 - 8192 (default: 0)",
    effectOnModel: "Limits tokens used for reasoning.",
    whenToAdjust: "Set to control reasoning length.",
  },
  custom_params: {
    title: "Custom Params",
    description: "Custom parameters in JSON format.",
    recommendedValue: "Valid JSON string",
    effectOnModel: "Passes custom parameters directly to model.",
    whenToAdjust: "Use for parameters not covered by standard settings.",
  },
};

export const uiTooltips: Record<string, TooltipContent> = {
  ...gpuTooltips,
  ...advancedTooltips,
};
