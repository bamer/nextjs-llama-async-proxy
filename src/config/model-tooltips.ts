import { TooltipContent } from "./tooltip-config.types";

export const samplingTooltips: Record<string, TooltipContent> = {
  temperature: {
    title: "Temperature",
    description: "Controls randomness in token selection. Higher values make output more random and creative, lower values make it more deterministic and focused.",
    recommendedValue: "0.0 - 2.0 (default: 0.7)",
    effectOnModel: "Higher values (≥1.0) increase creativity but may reduce coherence. Lower values (≤0.5) produce more predictable, focused responses.",
    whenToAdjust: "Increase for creative writing or brainstorming. Decrease for code generation, factual responses, or when you need precise outputs.",
  },
  top_k: {
    title: "Top K",
    description: "Limits token sampling to the K most likely tokens. Prevents the model from selecting from very low probability tokens.",
    recommendedValue: "1 - 100 (default: 40)",
    effectOnModel: "Lower values (1-10) restrict output to very likely tokens, reducing diversity. Higher values (40-100) allow more varied vocabulary.",
    whenToAdjust: "Use lower values for more deterministic outputs. Increase when the model is too repetitive or needs more vocabulary variety.",
  },
  top_p: {
    title: "Top P (Nucleus Sampling)",
    description: "Nucleus sampling: samples from the smallest set of tokens whose cumulative probability exceeds P. Works with Top K to control diversity.",
    recommendedValue: "0.1 - 1.0 (default: 0.9)",
    effectOnModel: "Lower values (0.1-0.5) create more focused, less diverse outputs. Higher values (0.8-1.0) allow more creative, varied responses.",
    whenToAdjust: "Decrease for more focused, predictable outputs. Increase for creative tasks where variety is desired.",
  },
  min_p: {
    title: "Min P",
    description: "Sets a minimum probability threshold for token selection. Tokens below this probability are excluded.",
    recommendedValue: "0.0 - 0.5 (default: 0.05)",
    effectOnModel: "Filters out very low probability tokens, reducing generation of nonsensical content.",
    whenToAdjust: "Increase to filter out more low-probability tokens and improve output quality.",
  },
  top_nsigma: {
    title: "Top N Sigma",
    description: "An alternative sampling method that selects tokens within N standard deviations from the mean.",
    recommendedValue: "0 - 3.0 (default: 0)",
    effectOnModel: "When enabled, provides an alternative to Top K/Top P with different distribution characteristics.",
    whenToAdjust: "Use when experimenting with different sampling strategies or standard Top K/Top P don't work well.",
  },
  xtc_probability: {
    title: "XTC Probability",
    description: "Probability of applying XTC (Excluding Top Combinations) sampling to reduce repetition.",
    recommendedValue: "0.0 - 1.0 (default: 0)",
    effectOnModel: "Reduces repetitive token patterns by excluding top combinations with some probability.",
    whenToAdjust: "Enable when the model produces repetitive content or loops.",
  },
  xtc_threshold: {
    title: "XTC Threshold",
    description: "Threshold for XTC sampling, controlling how aggressively to exclude top combinations.",
    recommendedValue: "0.0 - 1.0 (default: 0.1)",
    effectOnModel: "Higher thresholds exclude more top combinations, reducing repetition more aggressively.",
    whenToAdjust: "Adjust based on repetition levels in output.",
  },
  typical_p: {
    title: "Typical P",
    description: "Alternative sampling method that selects tokens based on typicality, filtering out both very high and very low probability tokens.",
    recommendedValue: "0.1 - 1.0 (default: 1.0)",
    effectOnModel: "Produces more natural, typical responses by avoiding both too-predictable and too-unlikely tokens.",
    whenToAdjust: "Use when Top P produces outputs that are either too predictable or too random.",
  },
  repeat_last_n: {
    title: "Repeat Last N",
    description: "Number of last tokens to consider when applying repeat penalties.",
    recommendedValue: "0 - 2048 (default: 64)",
    effectOnModel: "Controls the sliding window for detecting repeated patterns. Larger windows catch more distant repetitions.",
    whenToAdjust: "Increase to detect longer-range repetitions. Decrease for short-term repetition control.",
  },
  repeat_penalty: {
    title: "Repeat Penalty",
    description: "Applies a penalty to tokens that appear in recent context, reducing repetition.",
    recommendedValue: "1.0 - 2.0 (default: 1.0)",
    effectOnModel: "Values >1.0 penalize repeated tokens, reducing loops. 1.0 disables penalty. Too high values can break flow.",
    whenToAdjust: "Increase when model repeats itself. Decrease if output becomes unnatural or fragmented.",
  },
  presence_penalty: {
    title: "Presence Penalty",
    description: "Penalizes tokens that have already appeared at all in the generated text, encouraging variety.",
    recommendedValue: "0.0 - 2.0 (default: 0)",
    effectOnModel: "Encourages the model to talk about new topics and vocabulary. Too high can make responses incoherent.",
    whenToAdjust: "Use to encourage diverse vocabulary and avoid stuck topics.",
  },
  frequency_penalty: {
    title: "Frequency Penalty",
    description: "Penalizes tokens based on how frequently they've appeared, not just presence.",
    recommendedValue: "0.0 - 2.0 (default: 0)",
    effectOnModel: "More aggressive than presence penalty, heavily penalizing frequently used words.",
    whenToAdjust: "When presence penalty isn't enough to reduce word repetition.",
  },
  dry_multiplier: {
    title: "DRY Multiplier",
    description: "Multiplier for DRY (Don't Repeat Yourself) sampling to penalize repeated n-grams.",
    recommendedValue: "0.0 - 5.0 (default: 0)",
    effectOnModel: "Controls strength of DRY repetition penalty. Higher values reduce repetition more aggressively.",
    whenToAdjust: "Enable when the model repeats phrases or sentences.",
  },
  dry_base: {
    title: "DRY Base",
    description: "Base value for DRY penalty calculation.",
    recommendedValue: "1.0 - 3.0 (default: 1.75)",
    effectOnModel: "Adjusts the base penalty calculation for repeated content.",
    whenToAdjust: "Fine-tune in conjunction with DRY multiplier for optimal repetition control.",
  },
  dry_allowed_length: {
    title: "DRY Allowed Length",
    description: "Minimum length of repeated sequences to apply DRY penalty.",
    recommendedValue: "0 - 10 (default: 2)",
    effectOnModel: "Only penalizes repetitions longer than this value.",
    whenToAdjust: "Increase to allow short repetitions, decrease to catch more repetitions.",
  },
  dry_penalty_last_n: {
    title: "DRY Penalty Last N",
    description: "Number of tokens to look back when applying DRY penalty.",
    recommendedValue: "0 - 512 (default: 0)",
    effectOnModel: "Controls the context window for DRY repetition detection.",
    whenToAdjust: "Increase to detect longer-range repetitions.",
  },
  dry_sequence_breaker: {
    title: "DRY Sequence Breaker",
    description: "Characters that break sequences for DRY penalty calculation.",
    recommendedValue: "\\n, !, ., ? (default: \\n, !, ., ?)",
    effectOnModel: "Separates text into chunks at these characters when detecting repetitions.",
    whenToAdjust: "Customize based on language and text structure.",
  },
  dynatemp_range: {
    title: "Dynatemp Range",
    description: "Range for dynamic temperature adjustment during generation.",
    recommendedValue: "0.0 - 2.0 (default: 0)",
    effectOnModel: "Enables temperature to vary dynamically within this range for more varied outputs.",
    whenToAdjust: "Enable when you want varied creativity throughout the response.",
  },
  dynatemp_exponent: {
    title: "Dynatemp Exponent",
    description: "Exponent for dynamic temperature curve shape.",
    recommendedValue: "0.1 - 2.0 (default: 1)",
    effectOnModel: "Controls the distribution shape of dynamic temperature values.",
    whenToAdjust: "Fine-tune with dynatemp_range for desired dynamic behavior.",
  },
  mirostat: {
    title: "Mirostat",
    description: "Enables Mirostat algorithm for constant perplexity sampling. 0=off, 1=Mirostat, 2=Mirostat 2.0.",
    recommendedValue: "0, 1, or 2 (default: 0)",
    effectOnModel: "Maintains constant perplexity for more consistent text quality. Mirostat 2.0 is more advanced.",
    whenToAdjust: "Use when you need consistently high-quality output with controlled perplexity.",
  },
  mirostat_eta: {
    title: "Mirostat Eta (Learning Rate)",
    description: "Learning rate (eta) for Mirostat algorithm.",
    recommendedValue: "0.001 - 1.0 (default: 0.1)",
    effectOnModel: "Controls how quickly Mirostat adapts to maintain target perplexity.",
    whenToAdjust: "Adjust based on how quickly you want the algorithm to adapt.",
  },
  mirostat_tau: {
    title: "Mirostat Tau (Target Entropy)",
    description: "Target entropy (tau) for Mirostat algorithm.",
    recommendedValue: "0.0 - 10.0 (default: 5)",
    effectOnModel: "Sets the target perplexity/entropy for text generation.",
    whenToAdjust: "Lower for more focused text, higher for more diverse text.",
  },
  samplers: {
    title: "Samplers",
    description: "List of samplers to use, in order.",
    recommendedValue: "e.g., y_root, temp, top_p, top_k",
    effectOnModel: "Controls which sampling methods are applied and in what sequence.",
    whenToAdjust: "Customize for specific sampling strategies.",
  },
  sampler_seq: {
    title: "Sampler Sequence",
    description: "Alternative way to specify sampler order.",
    recommendedValue: "e.g., 0,1,2,3",
    effectOnModel: "Same as samplers but uses numeric IDs.",
    whenToAdjust: "Use when you prefer numeric sampler specification.",
  },
  seed: {
    title: "Seed",
    description: "Random seed for generation. -1 uses random seed.",
    recommendedValue: "-1 or positive integer (default: -1)",
    effectOnModel: "Same seed with same settings produces identical output.",
    whenToAdjust: "Set for reproducible outputs during testing or debugging.",
  },
  grammar: {
    title: "Grammar",
    description: "Grammar string in GBNF format to constrain output format.",
    recommendedValue: "Custom GBNF grammar",
    effectOnModel: "Forces output to conform to specified grammar (JSON, code formats, etc.)",
    whenToAdjust: "Use when you need structured output like JSON, code, or specific formats.",
  },
  grammar_file: {
    title: "Grammar File",
    description: "Path to a file containing GBNF grammar rules.",
    recommendedValue: "File path",
    effectOnModel: "Loads grammar from file instead of inline string.",
    whenToAdjust: "Use for complex grammars stored in files.",
  },
  json_schema: {
    title: "JSON Schema",
    description: "JSON schema to constrain output to valid JSON.",
    recommendedValue: "Valid JSON schema",
    effectOnModel: "Ensures output matches specified JSON schema.",
    whenToAdjust: "Use when you need JSON output with specific structure.",
  },
  json_schema_file: {
    title: "JSON Schema File",
    description: "Path to a file containing JSON schema.",
    recommendedValue: "File path",
    effectOnModel: "Loads JSON schema from file.",
    whenToAdjust: "Use for schemas stored in files.",
  },
  ignore_eos: {
    title: "Ignore EOS",
    description: "Whether to ignore end-of-sequence tokens. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "When enabled, model won't stop at EOS tokens, continuing generation.",
    whenToAdjust: "Enable when you want longer outputs beyond natural stopping points.",
  },
  escape: {
    title: "Escape",
    description: "Enables special character escaping in output.",
    recommendedValue: "false or true (default: false)",
    effectOnModel: "Affects how special characters are handled in output.",
    whenToAdjust: "Use when working with special characters or escape sequences.",
  },
};

export const ropeScalingTooltips: Record<string, TooltipContent> = {
  rope_scaling_type: {
    title: "ROPE Scaling Type",
    description: "Method for extending context window using ROPE (Rotary Position Embedding).",
    recommendedValue: "empty, linear, or yarn (default: empty)",
    effectOnModel: "Extends context window beyond model's original limit. YARN is more advanced.",
    whenToAdjust: "Use when you need longer context than the model natively supports.",
  },
  rope_scale: {
    title: "ROPE Scale",
    description: "Scaling factor for ROPE position embeddings.",
    recommendedValue: "0.0 - 10.0 (default: 0)",
    effectOnModel: "Scales the context window. 1.0 = no scaling.",
    whenToAdjust: "Set >1.0 to extend context window.",
  },
  rope_freq_base: {
    title: "ROPE Frequency Base",
    description: "Base frequency for ROPE position embeddings.",
    recommendedValue: "0 - 1000000 (default: 0)",
    effectOnModel: "Adjusts the frequency base for position encoding.",
    whenToAdjust: "Adjust in conjunction with ROPE scaling for optimal results.",
  },
  rope_freq_scale: {
    title: "ROPE Frequency Scale",
    description: "Frequency scaling factor for ROPE.",
    recommendedValue: "0.0 - 10.0 (default: 0)",
    effectOnModel: "Scales position frequencies for context extension.",
    whenToAdjust: "Use with ROPE scaling for fine-tuning context extension.",
  },
  yarn_orig_ctx: {
    title: "YARN Original Context",
    description: "Original context size for YARN scaling.",
    recommendedValue: "0 - 32768 (default: 0)",
    effectOnModel: "Sets the base context size for YARN scaling calculation.",
    whenToAdjust: "Set to model's original context size when using YARN.",
  },
  yarn_ext_factor: {
    title: "YARN Extension Factor",
    description: "Factor by which to extend context with YARN.",
    recommendedValue: "-1 to 16 (default: -1)",
    effectOnModel: "Controls context extension factor. -1 uses auto-detection.",
    whenToAdjust: "Set explicitly when auto-detection doesn't work well.",
  },
  yarn_attn_factor: {
    title: "YARN Attention Factor",
    description: "Attention factor for YARN scaling.",
    recommendedValue: "0.0 - 4.0 (default: 1)",
    effectOnModel: "Adjusts attention in extended context.",
    whenToAdjust: "Fine-tune for better performance on extended context.",
  },
  yarn_beta_slow: {
    title: "YARN Beta Slow",
    description: "Slow beta parameter for YARN.",
    recommendedValue: "0.0 - 10.0 (default: 1)",
    effectOnModel: "Controls slow component of YARN interpolation.",
    whenToAdjust: "Adjust for optimal extended context quality.",
  },
  yarn_beta_fast: {
    title: "YARN Beta Fast",
    description: "Fast beta parameter for YARN.",
    recommendedValue: "0.0 - 100.0 (default: 32)",
    effectOnModel: "Controls fast component of YARN interpolation.",
    whenToAdjust: "Adjust with yarn_beta_slow for best results.",
  },
  flash_attn: {
    title: "Flash Attention",
    description: "Enables Flash Attention implementation for faster inference. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: empty)",
    effectOnModel: "Significantly speeds up attention computation when supported by hardware.",
    whenToAdjust: "Enable if your GPU supports Flash Attention for faster generation.",
  },
  logit_bias: {
    title: "Logit Bias",
    description: "Comma-separated list of token biases in format token_id:bias.",
    recommendedValue: "e.g., 123:2.0,456:-1.0",
    effectOnModel: "Manually adjust probabilities of specific tokens.",
    whenToAdjust: "Use to encourage or discourage specific words or tokens.",
  },
};

export const loraTooltips: Record<string, TooltipContent> = {
  lora: {
    title: "LoRA Path",
    description: "Path to LoRA (Low-Rank Adaptation) adapter file.",
    recommendedValue: "File path to .gguf LoRA file",
    effectOnModel: "Applies LoRA adapter to modify model behavior and specialize it.",
    whenToAdjust: "Use when you want to fine-tune model for specific tasks or styles.",
  },
  lora_scaled: {
    title: "LoRA Scaled",
    description: "Path and scale for LoRA adapter in format path:scale.",
    recommendedValue: "e.g., adapter.gguf:1.0",
    effectOnModel: "Applies LoRA with specified scaling factor.",
    whenToAdjust: "Use to control LoRA influence strength.",
  },
  control_vector: {
    title: "Control Vector",
    description: "Path to control vector for steering model behavior.",
    recommendedValue: "File path to control vector",
    effectOnModel: "Steers model generation in specific directions or styles.",
    whenToAdjust: "Use for fine-grained control over output style or content.",
  },
  control_vector_scaled: {
    title: "Control Vector Scaled",
    description: "Control vector with scale in format path:scale.",
    recommendedValue: "e.g., vector.gguf:1.5",
    effectOnModel: "Applies control vector with specified strength.",
    whenToAdjust: "Adjust scale to control steering intensity.",
  },
  control_vector_layer_range: {
    title: "Control Vector Layer Range",
    description: "Layer range for control vector application.",
    recommendedValue: "e.g., 0,32",
    effectOnModel: "Limits control vector to specific layers.",
    whenToAdjust: "Restrict control to specific layers for targeted effects.",
  },
  model_draft: {
    title: "Model Draft",
    description: "Path to draft model for speculative decoding.",
    recommendedValue: "File path to draft model",
    effectOnModel: "Uses smaller, faster draft model for speculative decoding.",
    whenToAdjust: "Use for faster generation with minimal quality loss.",
  },
  model_url_draft: {
    title: "Model URL Draft",
    description: "URL to download draft model.",
    recommendedValue: "URL to draft model",
    effectOnModel: "Downloads draft model from URL for speculative decoding.",
    whenToAdjust: "Use when draft model needs to be downloaded.",
  },
  ctx_size_draft: {
    title: "Context Size Draft",
    description: "Context size for draft model in tokens.",
    recommendedValue: "512 - 16384 (default: 0)",
    effectOnModel: "Sets context window for draft model.",
    whenToAdjust: "Match to draft model's capabilities.",
  },
  threads_draft: {
    title: "Threads Draft",
    description: "Number of threads for draft model. -1 uses auto.",
    recommendedValue: "-1 or 1-32 (default: -1)",
    effectOnModel: "Controls parallelism for draft model processing.",
    whenToAdjust: "Adjust based on CPU cores and performance needs.",
  },
  threads_batch_draft: {
    title: "Threads Batch Draft",
    description: "Batch threads for draft model.",
    recommendedValue: "-1 or 1-32 (default: -1)",
    effectOnModel: "Controls batching parallelism for draft model.",
    whenToAdjust: "Adjust for optimal batch processing performance.",
  },
  draft_max: {
    title: "Draft Max",
    description: "Maximum number of draft tokens to generate.",
    recommendedValue: "1 - 64 (default: 16)",
    effectOnModel: "Limits speculative decoding draft length.",
    whenToAdjust: "Adjust based on acceptance rate and performance.",
  },
  draft_min: {
    title: "Draft Min",
    description: "Minimum number of draft tokens to generate.",
    recommendedValue: "1 - 32 (default: 5)",
    effectOnModel: "Ensures minimum draft length for speculative decoding.",
    whenToAdjust: "Set based on acceptance characteristics.",
  },
  draft_p_min: {
    title: "Draft P Min",
    description: "Minimum probability threshold for draft tokens.",
    recommendedValue: "0.0 - 0.5 (default: 0.05)",
    effectOnModel: "Filters low-probability draft tokens.",
    whenToAdjust: "Increase to improve draft quality.",
  },
  cache_type_k_draft: {
    title: "Cache Type K Draft",
    description: "Cache type for draft model K matrices.",
    recommendedValue: "e.g., f16, q8_0",
    effectOnModel: "Controls cache precision for draft model keys.",
    whenToAdjust: "Choose based on memory vs quality tradeoff.",
  },
  cache_type_v_draft: {
    title: "Cache Type V Draft",
    description: "Cache type for draft model V matrices.",
    recommendedValue: "e.g., f16, q8_0",
    effectOnModel: "Controls cache precision for draft model values.",
    whenToAdjust: "Choose based on memory vs quality tradeoff.",
  },
  cpu_moe_draft: {
    title: "CPU MoE Draft",
    description: "Whether draft model MoE runs on CPU. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Forces draft model MoE on CPU.",
    whenToAdjust: "Use when GPU memory is constrained.",
  },
  n_cpu_moe_draft: {
    title: "N CPU MoE Draft",
    description: "Number of MoE experts for draft on CPU.",
    recommendedValue: "0 - 32 (default: 0)",
    effectOnModel: "Limits MoE experts processed on CPU for draft model.",
    whenToAdjust: "Adjust based on CPU capacity.",
  },
  n_gpu_layers_draft: {
    title: "N GPU Layers Draft",
    description: "GPU layers for draft model. -1 uses auto.",
    recommendedValue: "-1 or 0-n (default: -1)",
    effectOnModel: "Controls draft model GPU offloading.",
    whenToAdjust: "Adjust based on available VRAM.",
  },
  device_draft: {
    title: "Device Draft",
    description: "Device for draft model.",
    recommendedValue: "e.g., cuda, metal, cpu",
    effectOnModel: "Selects compute backend for draft model.",
    whenToAdjust: "May differ from main model for optimization.",
  },
  spec_replace: {
    title: "Spec Replace",
    description: "Speculative decoding replacement strategy.",
    recommendedValue: "Replacement strategy",
    effectOnModel: "Controls how tokens are replaced during speculation.",
    whenToAdjust: "Configure based on speculative decoding behavior.",
  },
};

export const multimodalTooltips: Record<string, TooltipContent> = {
  mmproj: {
    title: "MMPROJ",
    description: "Path to multimodal projection model (CLIP encoder).",
    recommendedValue: "File path to mmproj .gguf file",
    effectOnModel: "Enables vision capabilities by loading projection model.",
    whenToAdjust: "Load when using vision features.",
  },
  mmproj_url: {
    title: "MMPROJ URL",
    description: "URL to download multimodal projection model.",
    recommendedValue: "URL to mmproj model",
    effectOnModel: "Downloads projection model for vision support.",
    whenToAdjust: "Use when projection model needs to be downloaded.",
  },
  mmproj_auto: {
    title: "MMPROJ Auto",
    description: "Whether to auto-detect multimodal model. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Automatically finds projection model.",
    whenToAdjust: "Enable for easier multimodal setup.",
  },
  mmproj_offload: {
    title: "MMPROJ Offload",
    description: "Whether to offload projection model to GPU. 0=off, 1=on.",
    recommendedValue: "0 or 1 (default: 0)",
    effectOnModel: "Offloads vision encoder to GPU for faster processing.",
    whenToAdjust: "Enable if GPU has sufficient VRAM for faster vision processing.",
  },
  image_min_tokens: {
    title: "Image Min Tokens",
    description: "Minimum number of tokens per image.",
    recommendedValue: "0 - 8192 (default: 0)",
    effectOnModel: "Controls minimum encoding length for images.",
    whenToAdjust: "Adjust based on image detail requirements.",
  },
  image_max_tokens: {
    title: "Image Max Tokens",
    description: "Maximum number of tokens per image.",
    recommendedValue: "0 - 8192 (default: 0)",
    effectOnModel: "Limits maximum encoding length for images.",
    whenToAdjust: "Increase for more detailed image understanding.",
  },
};

export const modelTooltips: Record<string, TooltipContent> = {
  ...samplingTooltips,
  ...ropeScalingTooltips,
  ...loraTooltips,
  ...multimodalTooltips,
};
