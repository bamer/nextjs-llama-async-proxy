LLAMA_CONFIG_VERSION = 1

[*]
model = 
ctx-size = 131000
threads = 0
temp = 0.7
n-gpu-layers = 0
batch = 512
ubatch = 512

[group tests]
model = 
_is_group = true

[IQuestLab.IQuest-Coder-V1-40B-Instruct.Q4_K_M.gguf]
model = /media/bamer/crucial MX300/llm/llama/models/IQuestLab.IQuest-Coder-V1-40B-Instruct.Q4_K_M.gguf/IQuestLab.IQuest-Coder-V1-40B-Instruct.Q4_K_M.gguf

[group tests/Ministral-3-14B-Reasoning-2512-Q4_1.gguf]
model = /media/bamer/crucial MX300/llm/llama/models/Ministral-3-14B-Reasoning-2512/Ministral-3-14B-Reasoning-2512-Q4_1.gguf

[group tests/Qwen3-14B-Q4_K_S.gguf]
model = /media/bamer/crucial MX300/llm/llama/models/Qwen3-14B-GGUF/Qwen3-14B-Q4_K_S.gguf

