LLAMA_CONFIG_VERSION = 1

[*]
ctx-size = 2048
temp = 0.7
n-gpu-layers = 0
threads = 0
batch = 512
ubatch = 512
split-mode = none
main-gpu = 0
seed = -1
load-on-startup = false

[qween type]
model = 

[jhjkhkjh]
model = 

[queen group]
model = 

[test group]
model = 

[new group tests]
model = 

[new group tests/fulllll]
model = 

