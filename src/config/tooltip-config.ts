import { ConfigType } from "@/components/ui/ModelConfigDialog";

export interface TooltipContent {
  title: string;
  description: string;
  recommendedValue?: string;
  effectOnModel?: string;
  whenToAdjust?: string;
}

export interface TooltipConfig {
  [configType: string]: {
    [fieldName: string]: TooltipContent;
  };
}

export const tooltipConfig: TooltipConfig = {
  sampling: {
    temperature: {
      title: "Temperature",
      description: "Controls randomness in token selection. Higher values make output more random and creative, lower values make it more deterministic and focused.",
      recommendedValue: "0.0 - 2.0 (default: 0.7)",
      effectOnModel: "Higher values (≥1.0) increase creativity but may reduce coherence. Lower values (≤0.5) produce more predictable, focused responses.",
      whenToAdjust: "Increase for creative writing or brainstorming. Decrease for code generation, factual responses, or when you need precise outputs.",
    },
    top_k: {
      title: "Top K",
      description: "Limits token sampling to the K most likely tokens. Prevents the model from selecting from very low probability tokens.",
      recommendedValue: "1 - 100 (default: 40)",
      effectOnModel: "Lower values (1-10) restrict output to very likely tokens, reducing diversity. Higher values (40-100) allow more varied vocabulary.",
      whenToAdjust: "Use lower values for more deterministic outputs. Increase when the model is too repetitive or needs more vocabulary variety.",
    },
    top_p: {
      title: "Top P (Nucleus Sampling)",
      description: "Nucleus sampling: samples from the smallest set of tokens whose cumulative probability exceeds P. Works with Top K to control diversity.",
      recommendedValue: "0.1 - 1.0 (default: 0.9)",
      effectOnModel: "Lower values (0.1-0.5) create more focused, less diverse outputs. Higher values (0.8-1.0) allow more creative, varied responses.",
      whenToAdjust: "Decrease for more focused, predictable outputs. Increase for creative tasks where variety is desired.",
    },
    min_p: {
      title: "Min P",
      description: "Sets a minimum probability threshold for token selection. Tokens below this probability are excluded.",
      recommendedValue: "0.0 - 0.5 (default: 0.05)",
      effectOnModel: "Filters out very low probability tokens, reducing generation of nonsensical content.",
      whenToAdjust: "Increase to filter out more low-probability tokens and improve output quality.",
    },
    top_nsigma: {
      title: "Top N Sigma",
      description: "An alternative sampling method that selects tokens within N standard deviations from the mean.",
      recommendedValue: "0 - 3.0 (default: 0)",
      effectOnModel: "When enabled, provides an alternative to Top K/Top P with different distribution characteristics.",
      whenToAdjust: "Use when experimenting with different sampling strategies or standard Top K/Top P don't work well.",
    },
    xtc_probability: {
      title: "XTC Probability",
      description: "Probability of applying XTC (Excluding Top Combinations) sampling to reduce repetition.",
      recommendedValue: "0.0 - 1.0 (default: 0)",
      effectOnModel: "Reduces repetitive token patterns by excluding top combinations with some probability.",
      whenToAdjust: "Enable when the model produces repetitive content or loops.",
    },
    xtc_threshold: {
      title: "XTC Threshold",
      description: "Threshold for XTC sampling, controlling how aggressively to exclude top combinations.",
      recommendedValue: "0.0 - 1.0 (default: 0.1)",
      effectOnModel: "Higher thresholds exclude more top combinations, reducing repetition more aggressively.",
      whenToAdjust: "Adjust based on repetition levels in output.",
    },
    typical_p: {
      title: "Typical P",
      description: "Alternative sampling method that selects tokens based on typicality, filtering out both very high and very low probability tokens.",
      recommendedValue: "0.1 - 1.0 (default: 1.0)",
      effectOnModel: "Produces more natural, typical responses by avoiding both too-predictable and too-unlikely tokens.",
      whenToAdjust: "Use when Top P produces outputs that are either too predictable or too random.",
    },
    repeat_last_n: {
      title: "Repeat Last N",
      description: "Number of last tokens to consider when applying repeat penalties.",
      recommendedValue: "0 - 2048 (default: 64)",
      effectOnModel: "Controls the sliding window for detecting repeated patterns. Larger windows catch more distant repetitions.",
      whenToAdjust: "Increase to detect longer-range repetitions. Decrease for short-term repetition control.",
    },
    repeat_penalty: {
      title: "Repeat Penalty",
      description: "Applies a penalty to tokens that appear in recent context, reducing repetition.",
      recommendedValue: "1.0 - 2.0 (default: 1.0)",
      effectOnModel: "Values >1.0 penalize repeated tokens, reducing loops. 1.0 disables penalty. Too high values can break flow.",
      whenToAdjust: "Increase when model repeats itself. Decrease if output becomes unnatural or fragmented.",
    },
    presence_penalty: {
      title: "Presence Penalty",
      description: "Penalizes tokens that have already appeared at all in the generated text, encouraging variety.",
      recommendedValue: "0.0 - 2.0 (default: 0)",
      effectOnModel: "Encourages the model to talk about new topics and vocabulary. Too high can make responses incoherent.",
      whenToAdjust: "Use to encourage diverse vocabulary and avoid stuck topics.",
    },
    frequency_penalty: {
      title: "Frequency Penalty",
      description: "Penalizes tokens based on how frequently they've appeared, not just presence.",
      recommendedValue: "0.0 - 2.0 (default: 0)",
      effectOnModel: "More aggressive than presence penalty, heavily penalizing frequently used words.",
      whenToAdjust: "When presence penalty isn't enough to reduce word repetition.",
    },
    dry_multiplier: {
      title: "DRY Multiplier",
      description: "Multiplier for DRY (Don't Repeat Yourself) sampling to penalize repeated n-grams.",
      recommendedValue: "0.0 - 5.0 (default: 0)",
      effectOnModel: "Controls strength of DRY repetition penalty. Higher values reduce repetition more aggressively.",
      whenToAdjust: "Enable when the model repeats phrases or sentences.",
    },
    dry_base: {
      title: "DRY Base",
      description: "Base value for DRY penalty calculation.",
      recommendedValue: "1.0 - 3.0 (default: 1.75)",
      effectOnModel: "Adjusts the base penalty calculation for repeated content.",
      whenToAdjust: "Fine-tune in conjunction with DRY multiplier for optimal repetition control.",
    },
    dry_allowed_length: {
      title: "DRY Allowed Length",
      description: "Minimum length of repeated sequences to apply DRY penalty.",
      recommendedValue: "0 - 10 (default: 2)",
      effectOnModel: "Only penalizes repetitions longer than this value.",
      whenToAdjust: "Increase to allow short repetitions, decrease to catch more repetitions.",
    },
    dry_penalty_last_n: {
      title: "DRY Penalty Last N",
      description: "Number of tokens to look back when applying DRY penalty.",
      recommendedValue: "0 - 512 (default: 0)",
      effectOnModel: "Controls the context window for DRY repetition detection.",
      whenToAdjust: "Increase to detect longer-range repetitions.",
    },
    dry_sequence_breaker: {
      title: "DRY Sequence Breaker",
      description: "Characters that break sequences for DRY penalty calculation.",
      recommendedValue: "\\n, !, ., ? (default: \\n, !, ., ?)",
      effectOnModel: "Separates text into chunks at these characters when detecting repetitions.",
      whenToAdjust: "Customize based on language and text structure.",
    },
    dynatemp_range: {
      title: "Dynatemp Range",
      description: "Range for dynamic temperature adjustment during generation.",
      recommendedValue: "0.0 - 2.0 (default: 0)",
      effectOnModel: "Enables temperature to vary dynamically within this range for more varied outputs.",
      whenToAdjust: "Enable when you want varied creativity throughout the response.",
    },
    dynatemp_exp: {
      title: "Dynatemp Exp",
      description: "Exponent for dynamic temperature curve shape.",
      recommendedValue: "0.1 - 2.0 (default: 1)",
      effectOnModel: "Controls the distribution shape of dynamic temperature values.",
      whenToAdjust: "Fine-tune with dynatemp_range for desired dynamic behavior.",
    },
    mirostat: {
      title: "Mirostat",
      description: "Enables Mirostat algorithm for constant perplexity sampling. 0=off, 1=Mirostat, 2=Mirostat 2.0.",
      recommendedValue: "0, 1, or 2 (default: 0)",
      effectOnModel: "Maintains constant perplexity for more consistent text quality. Mirostat 2.0 is more advanced.",
      whenToAdjust: "Use when you need consistently high-quality output with controlled perplexity.",
    },
    mirostat_lr: {
      title: "Mirostat Learning Rate",
      description: "Learning rate for Mirostat algorithm.",
      recommendedValue: "0.001 - 1.0 (default: 0.1)",
      effectOnModel: "Controls how quickly Mirostat adapts to maintain target perplexity.",
      whenToAdjust: "Adjust based on how quickly you want the algorithm to adapt.",
    },
    mirostat_ent: {
      title: "Mirostat Entropy",
      description: "Target entropy for Mirostat algorithm.",
      recommendedValue: "0.0 - 10.0 (default: 5)",
      effectOnModel: "Sets the target perplexity/entropy for text generation.",
      whenToAdjust: "Lower for more focused text, higher for more diverse text.",
    },
    samplers: {
      title: "Samplers",
      description: "List of samplers to use, in order.",
      recommendedValue: "e.g., y_root, temp, top_p, top_k",
      effectOnModel: "Controls which sampling methods are applied and in what sequence.",
      whenToAdjust: "Customize for specific sampling strategies.",
    },
    sampler_seq: {
      title: "Sampler Sequence",
      description: "Alternative way to specify sampler order.",
      recommendedValue: "e.g., 0,1,2,3",
      effectOnModel: "Same as samplers but uses numeric IDs.",
      whenToAdjust: "Use when you prefer numeric sampler specification.",
    },
    seed: {
      title: "Seed",
      description: "Random seed for generation. -1 uses random seed.",
      recommendedValue: "-1 or positive integer (default: -1)",
      effectOnModel: "Same seed with same settings produces identical output.",
      whenToAdjust: "Set for reproducible outputs during testing or debugging.",
    },
    grammar: {
      title: "Grammar",
      description: "Grammar string in GBNF format to constrain output format.",
      recommendedValue: "Custom GBNF grammar",
      effectOnModel: "Forces output to conform to specified grammar (JSON, code formats, etc.)",
      whenToAdjust: "Use when you need structured output like JSON, code, or specific formats.",
    },
    grammar_file: {
      title: "Grammar File",
      description: "Path to a file containing GBNF grammar rules.",
      recommendedValue: "File path",
      effectOnModel: "Loads grammar from file instead of inline string.",
      whenToAdjust: "Use for complex grammars stored in files.",
    },
    json_schema: {
      title: "JSON Schema",
      description: "JSON schema to constrain output to valid JSON.",
      recommendedValue: "Valid JSON schema",
      effectOnModel: "Ensures output matches specified JSON schema.",
      whenToAdjust: "Use when you need JSON output with specific structure.",
    },
    json_schema_file: {
      title: "JSON Schema File",
      description: "Path to a file containing JSON schema.",
      recommendedValue: "File path",
      effectOnModel: "Loads JSON schema from file.",
      whenToAdjust: "Use for schemas stored in files.",
    },
    ignore_eos: {
      title: "Ignore EOS",
      description: "Whether to ignore end-of-sequence tokens. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "When enabled, model won't stop at EOS tokens, continuing generation.",
      whenToAdjust: "Enable when you want longer outputs beyond natural stopping points.",
    },
    escape: {
      title: "Escape",
      description: "Enables special character escaping in output.",
      recommendedValue: "false or true (default: false)",
      effectOnModel: "Affects how special characters are handled in output.",
      whenToAdjust: "Use when working with special characters or escape sequences.",
    },
    rope_scaling_type: {
      title: "ROPE Scaling Type",
      description: "Method for extending context window using ROPE (Rotary Position Embedding).",
      recommendedValue: "empty, linear, or yarn (default: empty)",
      effectOnModel: "Extends context window beyond model's original limit. YARN is more advanced.",
      whenToAdjust: "Use when you need longer context than the model natively supports.",
    },
    rope_scale: {
      title: "ROPE Scale",
      description: "Scaling factor for ROPE position embeddings.",
      recommendedValue: "0.0 - 10.0 (default: 0)",
      effectOnModel: "Scales the context window. 1.0 = no scaling.",
      whenToAdjust: "Set >1.0 to extend context window.",
    },
    rope_freq_base: {
      title: "ROPE Frequency Base",
      description: "Base frequency for ROPE position embeddings.",
      recommendedValue: "0 - 1000000 (default: 0)",
      effectOnModel: "Adjusts the frequency base for position encoding.",
      whenToAdjust: "Adjust in conjunction with ROPE scaling for optimal results.",
    },
    rope_freq_scale: {
      title: "ROPE Frequency Scale",
      description: "Frequency scaling factor for ROPE.",
      recommendedValue: "0.0 - 10.0 (default: 0)",
      effectOnModel: "Scales position frequencies for context extension.",
      whenToAdjust: "Use with ROPE scaling for fine-tuning context extension.",
    },
    yarn_orig_ctx: {
      title: "YARN Original Context",
      description: "Original context size for YARN scaling.",
      recommendedValue: "0 - 32768 (default: 0)",
      effectOnModel: "Sets the base context size for YARN scaling calculation.",
      whenToAdjust: "Set to model's original context size when using YARN.",
    },
    yarn_ext_factor: {
      title: "YARN Extension Factor",
      description: "Factor by which to extend context with YARN.",
      recommendedValue: "-1 to 16 (default: -1)",
      effectOnModel: "Controls context extension factor. -1 uses auto-detection.",
      whenToAdjust: "Set explicitly when auto-detection doesn't work well.",
    },
    yarn_attn_factor: {
      title: "YARN Attention Factor",
      description: "Attention factor for YARN scaling.",
      recommendedValue: "0.0 - 4.0 (default: 1)",
      effectOnModel: "Adjusts attention in extended context.",
      whenToAdjust: "Fine-tune for better performance on extended context.",
    },
    yarn_beta_slow: {
      title: "YARN Beta Slow",
      description: "Slow beta parameter for YARN.",
      recommendedValue: "0.0 - 10.0 (default: 1)",
      effectOnModel: "Controls slow component of YARN interpolation.",
      whenToAdjust: "Adjust for optimal extended context quality.",
    },
    yarn_beta_fast: {
      title: "YARN Beta Fast",
      description: "Fast beta parameter for YARN.",
      recommendedValue: "0.0 - 100.0 (default: 32)",
      effectOnModel: "Controls fast component of YARN interpolation.",
      whenToAdjust: "Adjust with yarn_beta_slow for best results.",
    },
    flash_attn: {
      title: "Flash Attention",
      description: "Enables Flash Attention implementation for faster inference. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: empty)",
      effectOnModel: "Significantly speeds up attention computation when supported by hardware.",
      whenToAdjust: "Enable if your GPU supports Flash Attention for faster generation.",
    },
    logit_bias: {
      title: "Logit Bias",
      description: "Comma-separated list of token biases in format token_id:bias.",
      recommendedValue: "e.g., 123:2.0,456:-1.0",
      effectOnModel: "Manually adjust probabilities of specific tokens.",
      whenToAdjust: "Use to encourage or discourage specific words or tokens.",
    },
  },
  memory: {
    cache_ram: {
      title: "Cache RAM",
      description: "Amount of RAM to allocate for KV cache in GB. 0 uses automatic sizing.",
      recommendedValue: "0 or positive GB (default: 0)",
      effectOnModel: "Controls how much model context is kept in fast RAM.",
      whenToAdjust: "Set manually if automatic sizing isn't optimal for your system.",
    },
    cache_type_k: {
      title: "Cache Type K",
      description: "Type of cache for K (key) matrices.",
      recommendedValue: "e.g., f16, q8_0, q4_0",
      effectOnModel: "Determines precision and memory usage for key cache.",
      whenToAdjust: "Choose based on memory constraints and quality requirements.",
    },
    cache_type_v: {
      title: "Cache Type V",
      description: "Type of cache for V (value) matrices.",
      recommendedValue: "e.g., f16, q8_0, q4_0",
      effectOnModel: "Determines precision and memory usage for value cache.",
      whenToAdjust: "Choose based on memory constraints and quality requirements.",
    },
    mmap: {
      title: "Memory Map",
      description: "Whether to use memory-mapped files for model weights. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 1)",
      effectOnModel: "MMap reduces RAM usage but may be slower. Disabling loads model entirely into RAM.",
      whenToAdjust: "Disable for maximum speed if you have enough RAM. Enable to save memory.",
    },
    mlock: {
      title: "Memory Lock",
      description: "Whether to lock model in physical RAM to prevent swapping. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Prevents model from being swapped to disk, improving performance.",
      whenToAdjust: "Enable if system is swapping model to disk and degrading performance.",
    },
    numa: {
      title: "NUMA",
      description: "NUMA policy for multi-socket systems.",
      recommendedValue: "e.g., interleave, preferred",
      effectOnModel: "Optimizes memory access on NUMA architectures.",
      whenToAdjust: "Configure for multi-socket systems for optimal performance.",
    },
    defrag_thold: {
      title: "Defrag Threshold",
      description: "Cache defragmentation threshold. -1 disables defrag.",
      recommendedValue: "-1 or 0-1.0 (default: -1)",
      effectOnModel: "When cache fragmentation exceeds threshold, defrag runs.",
      whenToAdjust: "Enable if experiencing performance degradation due to fragmentation.",
    },
  },
  gpu: {
    device: {
      title: "Device",
      description: "Device to use for computation.",
      recommendedValue: "e.g., cuda, metal, cpu",
      effectOnModel: "Selects compute backend. CUDA for NVIDIA, Metal for Apple Silicon.",
      whenToAdjust: "Set based on your hardware. Auto-detection usually works.",
    },
    list_devices: {
      title: "List Devices",
      description: "Whether to list available devices. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "When enabled, logs available devices.",
      whenToAdjust: "Use for debugging device selection.",
    },
    gpu_layers: {
      title: "GPU Layers",
      description: "Number of model layers to offload to GPU. -1 offloads all possible layers.",
      recommendedValue: "-1 or 0-n (default: -1)",
      effectOnModel: "More layers = faster inference but more VRAM used.",
      whenToAdjust: "Decrease if you run out of VRAM. Increase for maximum speed.",
    },
    split_mode: {
      title: "Split Mode",
      description: "How to split model across multiple GPUs.",
      recommendedValue: "empty, layer, or row",
      effectOnModel: "Layer splits by layers, row splits tensors within layers.",
      whenToAdjust: "Configure for multi-GPU setups to optimize VRAM usage.",
    },
    tensor_split: {
      title: "Tensor Split",
      description: "Comma-separated list of VRAM allocations for each GPU.",
      recommendedValue: "e.g., 8,8,8 for 8GB each on 3 GPUs",
      effectOnModel: "Controls VRAM allocation per GPU in multi-GPU setups.",
      whenToAdjust: "Set based on each GPU's VRAM capacity.",
    },
    main_gpu: {
      title: "Main GPU",
      description: "Primary GPU for main model operations.",
      recommendedValue: "GPU index (default: 0)",
      effectOnModel: "Specifies which GPU handles the main computation.",
      whenToAdjust: "Set to fastest GPU in heterogeneous multi-GPU setups.",
    },
    kv_offload: {
      title: "KV Offload",
      description: "Whether to offload KV cache to GPU. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Offloading KV cache to GPU improves speed but uses VRAM.",
      whenToAdjust: "Enable if you have spare VRAM for faster inference.",
    },
    repack: {
      title: "Repack",
      description: "Whether to repack tensors for better GPU utilization. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Repacking can improve performance but adds startup time.",
      whenToAdjust: "Enable if startup time isn't critical and you want max performance.",
    },
    no_host: {
      title: "No Host",
      description: "Whether to disable host memory usage. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "When enabled, forces all computation on device.",
      whenToAdjust: "Use only if device has enough memory for entire model.",
    },
  },
  advanced: {
    swa_full: {
      title: "SWA Full",
      description: "Sliding Window Attention full mode. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Enables full sliding window attention for better context handling.",
      whenToAdjust: "Enable for models that support SWA for improved performance.",
    },
    override_tensor: {
      title: "Override Tensor",
      description: "Override specific tensor settings.",
      recommendedValue: "Tensor specification",
      effectOnModel: "Manually override tensor configuration.",
      whenToAdjust: "For advanced debugging or optimization.",
    },
    cpu_moe: {
      title: "CPU MoE",
      description: "Whether to handle Mixture of Experts on CPU. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Forces MoE computation on CPU instead of GPU.",
      whenToAdjust: "Use when GPU memory is limited or for MoE models.",
    },
    n_cpu_moe: {
      title: "N CPU MoE",
      description: "Number of MoE experts to handle on CPU.",
      recommendedValue: "0 - 64 (default: 0)",
      effectOnModel: "Limits number of experts processed on CPU.",
      whenToAdjust: "Adjust based on CPU capabilities.",
    },
    kv_unified: {
      title: "KV Unified",
      description: "Unified KV cache mode. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Uses unified cache for keys and values.",
      whenToAdjust: "Enable if supported by model for memory savings.",
    },
    pooling: {
      title: "Pooling",
      description: "Pooling layer type for embedding models.",
      recommendedValue: "e.g., mean, cls, last",
      effectOnModel: "Determines how tokens are pooled into embeddings.",
      whenToAdjust: "Set based on model architecture and use case.",
    },
    context_shift: {
      title: "Context Shift",
      description: "Enables context shift for very long sequences. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Allows handling sequences longer than model's context window.",
      whenToAdjust: "Enable when working with very long inputs.",
    },
    rpc: {
      title: "RPC",
      description: "RPC server configuration for distributed inference.",
      recommendedValue: "RPC configuration string",
      effectOnModel: "Enables distributed inference across machines.",
      whenToAdjust: "Configure for distributed setups.",
    },
    offline: {
      title: "Offline",
      description: "Offline mode. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Disables certain online features or checks.",
      whenToAdjust: "Use in air-gapped environments or when network access is restricted.",
    },
    override_kv: {
      title: "Override KV",
      description: "Override KV cache settings.",
      recommendedValue: "KV cache specification",
      effectOnModel: "Manually configure KV cache behavior.",
      whenToAdjust: "For advanced memory optimization.",
    },
    op_offload: {
      title: "Op Offload",
      description: "Operation offload configuration. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Offloads specific operations to device.",
      whenToAdjust: "Enable for operations that benefit from GPU acceleration.",
    },
    fit: {
      title: "Fit",
      description: "Model fit strategy.",
      recommendedValue: "e.g., context, speed",
      effectOnModel: "Controls how model fits in available memory.",
      whenToAdjust: "Set based on whether you prioritize context length or speed.",
    },
    fit_target: {
      title: "Fit Target",
      description: "Target for model fitting.",
      recommendedValue: "0 - 100 (default: 0)",
      effectOnModel: "Sets optimization target for fitting.",
      whenToAdjust: "Adjust based on memory availability.",
    },
    fit_ctx: {
      title: "Fit Context",
      description: "Context size target for model fitting.",
      recommendedValue: "0 - 32768 (default: 0)",
      effectOnModel: "Sets target context length when fitting model.",
      whenToAdjust: "Set to desired context length.",
    },
    check_tensors: {
      title: "Check Tensors",
      description: "Whether to validate tensors. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Performs tensor validation checks.",
      whenToAdjust: "Enable for debugging model loading issues.",
    },
    sleep_idle_seconds: {
      title: "Sleep Idle Seconds",
      description: "Seconds to sleep when idle before resource cleanup.",
      recommendedValue: "0 - 3600 (default: 0)",
      effectOnModel: "Controls idle timeout for resource management.",
      whenToAdjust: "Set to free resources after inactivity.",
    },
    polling: {
      title: "Polling",
      description: "Polling configuration.",
      recommendedValue: "Polling configuration string",
      effectOnModel: "Controls polling behavior for async operations.",
      whenToAdjust: "Configure based on async handling requirements.",
    },
    polling_batch: {
      title: "Polling Batch",
      description: "Batch polling configuration.",
      recommendedValue: "Batch polling string",
      effectOnModel: "Controls how batches are polled.",
      whenToAdjust: "Configure for optimal batch processing.",
    },
    reasoning_format: {
      title: "Reasoning Format",
      description: "Format for reasoning outputs.",
      recommendedValue: "e.g., think, chain",
      effectOnModel: "Controls how reasoning is formatted in output.",
      whenToAdjust: "Set based on desired reasoning output format.",
    },
    reasoning_budget: {
      title: "Reasoning Budget",
      description: "Token budget for reasoning.",
      recommendedValue: "0 - 8192 (default: 0)",
      effectOnModel: "Limits tokens used for reasoning.",
      whenToAdjust: "Set to control reasoning length.",
    },
    custom_params: {
      title: "Custom Params",
      description: "Custom parameters in JSON format.",
      recommendedValue: "Valid JSON string",
      effectOnModel: "Passes custom parameters directly to model.",
      whenToAdjust: "Use for parameters not covered by standard settings.",
    },
  },
  lora: {
    lora: {
      title: "LoRA Path",
      description: "Path to LoRA (Low-Rank Adaptation) adapter file.",
      recommendedValue: "File path to .gguf LoRA file",
      effectOnModel: "Applies LoRA adapter to modify model behavior and specialize it.",
      whenToAdjust: "Use when you want to fine-tune model for specific tasks or styles.",
    },
    lora_scaled: {
      title: "LoRA Scaled",
      description: "Path and scale for LoRA adapter in format path:scale.",
      recommendedValue: "e.g., adapter.gguf:1.0",
      effectOnModel: "Applies LoRA with specified scaling factor.",
      whenToAdjust: "Use to control LoRA influence strength.",
    },
    control_vector: {
      title: "Control Vector",
      description: "Path to control vector for steering model behavior.",
      recommendedValue: "File path to control vector",
      effectOnModel: "Steers model generation in specific directions or styles.",
      whenToAdjust: "Use for fine-grained control over output style or content.",
    },
    control_vector_scaled: {
      title: "Control Vector Scaled",
      description: "Control vector with scale in format path:scale.",
      recommendedValue: "e.g., vector.gguf:1.5",
      effectOnModel: "Applies control vector with specified strength.",
      whenToAdjust: "Adjust scale to control steering intensity.",
    },
    control_vector_layer_range: {
      title: "Control Vector Layer Range",
      description: "Layer range for control vector application.",
      recommendedValue: "e.g., 0,32",
      effectOnModel: "Limits control vector to specific layers.",
      whenToAdjust: "Restrict control to specific layers for targeted effects.",
    },
    model_draft: {
      title: "Model Draft",
      description: "Path to draft model for speculative decoding.",
      recommendedValue: "File path to draft model",
      effectOnModel: "Uses smaller, faster draft model for speculative decoding.",
      whenToAdjust: "Use for faster generation with minimal quality loss.",
    },
    model_url_draft: {
      title: "Model URL Draft",
      description: "URL to download draft model.",
      recommendedValue: "URL to draft model",
      effectOnModel: "Downloads draft model from URL for speculative decoding.",
      whenToAdjust: "Use when draft model needs to be downloaded.",
    },
    ctx_size_draft: {
      title: "Context Size Draft",
      description: "Context size for draft model in tokens.",
      recommendedValue: "512 - 16384 (default: 0)",
      effectOnModel: "Sets context window for draft model.",
      whenToAdjust: "Match to draft model's capabilities.",
    },
    threads_draft: {
      title: "Threads Draft",
      description: "Number of threads for draft model. -1 uses auto.",
      recommendedValue: "-1 or 1-32 (default: -1)",
      effectOnModel: "Controls parallelism for draft model processing.",
      whenToAdjust: "Adjust based on CPU cores and performance needs.",
    },
    threads_batch_draft: {
      title: "Threads Batch Draft",
      description: "Batch threads for draft model.",
      recommendedValue: "-1 or 1-32 (default: -1)",
      effectOnModel: "Controls batching parallelism for draft model.",
      whenToAdjust: "Adjust for optimal batch processing performance.",
    },
    draft_max: {
      title: "Draft Max",
      description: "Maximum number of draft tokens to generate.",
      recommendedValue: "1 - 64 (default: 16)",
      effectOnModel: "Limits speculative decoding draft length.",
      whenToAdjust: "Adjust based on acceptance rate and performance.",
    },
    draft_min: {
      title: "Draft Min",
      description: "Minimum number of draft tokens to generate.",
      recommendedValue: "1 - 32 (default: 5)",
      effectOnModel: "Ensures minimum draft length for speculative decoding.",
      whenToAdjust: "Set based on acceptance characteristics.",
    },
    draft_p_min: {
      title: "Draft P Min",
      description: "Minimum probability threshold for draft tokens.",
      recommendedValue: "0.0 - 0.5 (default: 0.05)",
      effectOnModel: "Filters low-probability draft tokens.",
      whenToAdjust: "Increase to improve draft quality.",
    },
    cache_type_k_draft: {
      title: "Cache Type K Draft",
      description: "Cache type for draft model K matrices.",
      recommendedValue: "e.g., f16, q8_0",
      effectOnModel: "Controls cache precision for draft model keys.",
      whenToAdjust: "Choose based on memory vs quality tradeoff.",
    },
    cache_type_v_draft: {
      title: "Cache Type V Draft",
      description: "Cache type for draft model V matrices.",
      recommendedValue: "e.g., f16, q8_0",
      effectOnModel: "Controls cache precision for draft model values.",
      whenToAdjust: "Choose based on memory vs quality tradeoff.",
    },
    cpu_moe_draft: {
      title: "CPU MoE Draft",
      description: "Whether draft model MoE runs on CPU. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Forces draft model MoE on CPU.",
      whenToAdjust: "Use when GPU memory is constrained.",
    },
    n_cpu_moe_draft: {
      title: "N CPU MoE Draft",
      description: "Number of MoE experts for draft on CPU.",
      recommendedValue: "0 - 32 (default: 0)",
      effectOnModel: "Limits MoE experts processed on CPU for draft model.",
      whenToAdjust: "Adjust based on CPU capacity.",
    },
    n_gpu_layers_draft: {
      title: "N GPU Layers Draft",
      description: "GPU layers for draft model. -1 uses auto.",
      recommendedValue: "-1 or 0-n (default: -1)",
      effectOnModel: "Controls draft model GPU offloading.",
      whenToAdjust: "Adjust based on available VRAM.",
    },
    device_draft: {
      title: "Device Draft",
      description: "Device for draft model.",
      recommendedValue: "e.g., cuda, metal, cpu",
      effectOnModel: "Selects compute backend for draft model.",
      whenToAdjust: "May differ from main model for optimization.",
    },
    spec_replace: {
      title: "Spec Replace",
      description: "Speculative decoding replacement strategy.",
      recommendedValue: "Replacement strategy",
      effectOnModel: "Controls how tokens are replaced during speculation.",
      whenToAdjust: "Configure based on speculative decoding behavior.",
    },
  },
  multimodal: {
    mmproj: {
      title: "MMPROJ",
      description: "Path to multimodal projection model (CLIP encoder).",
      recommendedValue: "File path to mmproj .gguf file",
      effectOnModel: "Enables vision capabilities by loading projection model.",
      whenToAdjust: "Load when using vision features.",
    },
    mmproj_url: {
      title: "MMPROJ URL",
      description: "URL to download multimodal projection model.",
      recommendedValue: "URL to mmproj model",
      effectOnModel: "Downloads projection model for vision support.",
      whenToAdjust: "Use when projection model needs to be downloaded.",
    },
    mmproj_auto: {
      title: "MMPROJ Auto",
      description: "Whether to auto-detect multimodal model. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Automatically finds projection model.",
      whenToAdjust: "Enable for easier multimodal setup.",
    },
    mmproj_offload: {
      title: "MMPROJ Offload",
      description: "Whether to offload projection model to GPU. 0=off, 1=on.",
      recommendedValue: "0 or 1 (default: 0)",
      effectOnModel: "Offloads vision encoder to GPU for faster processing.",
      whenToAdjust: "Enable if GPU has sufficient VRAM for faster vision processing.",
    },
    image_min_tokens: {
      title: "Image Min Tokens",
      description: "Minimum number of tokens per image.",
      recommendedValue: "0 - 8192 (default: 0)",
      effectOnModel: "Controls minimum encoding length for images.",
      whenToAdjust: "Adjust based on image detail requirements.",
    },
    image_max_tokens: {
      title: "Image Max Tokens",
      description: "Maximum number of tokens per image.",
      recommendedValue: "0 - 8192 (default: 0)",
      effectOnModel: "Limits maximum encoding length for images.",
      whenToAdjust: "Increase for more detailed image understanding.",
    },
  },
};

export function getTooltipContent(configType: ConfigType, fieldName: string): TooltipContent | undefined {
  return tooltipConfig[configType]?.[fieldName];
}
