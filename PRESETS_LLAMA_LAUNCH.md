# Launch Llama Server with Presets

Integration guide for launching `llama-server` in router mode using preset configuration files generated by the Presets page.

## Overview

The Presets page generates `.ini` configuration files that define llama.cpp router mode model configurations. Now you can launch `llama-server` directly with these preset files from the dashboard.

## Features

- **Preset-Based Launch**: Start llama-server with a complete model configuration file
- **Dual Mode**: Supports both `--models-dir` (auto-discovery) and `--models-preset` (config file) modes
- **Zero Configuration**: No need to manually create command-line arguments
- **Dashboard Integration**: Control llama-server startup directly from the UI

## Architecture

### Backend Changes

#### 1. Enhanced Router Start (`server/handlers/llama-router/start.js`)

The `startLlamaServerRouter()` function now supports both modes:

```javascript
// Directory mode (auto-discovery) - existing behavior
startLlamaServerRouter("/path/to/models", db, { maxModels: 4 });

// Preset mode - new behavior
startLlamaServerRouter("/path/to/config/my-preset.ini", db, {
  maxModels: 4,
  usePreset: true  // Flag to use preset mode
});
```

**Smart Detection**: The function automatically detects if the path is a `.ini` file and switches to preset mode.

```javascript
const isPresetFile = modelsDir.endsWith(".ini") || options.usePreset;
if (isPresetFile) {
  args.push("--models-preset", modelsDir);
} else {
  args.push("--models-dir", modelsDir);
}
```

#### 2. New Socket.IO Handlers (`server/handlers/presets.js`)

**`presets:start-with-preset`**
```javascript
// Request
{
  filename: "production",  // Preset name (without .ini)
  options: {
    maxModels: 4,
    threads: 4,
    ctxSize: 4096
  }
}

// Response (success)
{
  success: true,
  data: {
    port: 8080,
    url: "http://127.0.0.1:8080",
    mode: "router",
    preset: "production"
  }
}

// Response (error)
{
  success: false,
  error: { message: "Preset file not found: production" }
}
```

**`presets:stop-server`**
```javascript
// Request - no parameters needed
{}

// Response (success)
{
  success: true,
  data: { /* stop details */ }
}
```

## Usage Flow

### 1. Create a Preset

```javascript
// Create empty preset
await stateManager.request("presets:create", { filename: "my-preset" });

// Add models to preset
await stateManager.request("presets:add-model", {
  filename: "my-preset",
  modelName: "model-name",
  config: {
    model: "/path/to/model.gguf",
    ctxSize: 2048,
    nGpuLayers: 33,
    temperature: 0.7
  }
});

// Save preset
await stateManager.request("presets:save", {
  filename: "my-preset",
  config: { /* full config object */ }
});
```

### 2. Launch llama-server with Preset

```javascript
// Start llama-server with the preset
const response = await stateManager.request("presets:start-with-preset", {
  filename: "my-preset",
  options: {
    maxModels: 4,
    threads: 4,
    ctxSize: 4096
  }
});

if (response.success) {
  console.log("Server started on port", response.data.port);
} else {
  console.error("Failed:", response.error.message);
}
```

### 3. Stop llama-server

```javascript
const response = await stateManager.request("presets:stop-server");
if (response.success) {
  console.log("Server stopped");
}
```

## INI Preset Format

### Example Preset File

```ini
LLAMA_CONFIG_VERSION = 1

[*]
ctx-size = 4096
n-gpu-layers = 33
threads = 8

[llama2-7b]
model = /models/llama2-7b.gguf
temperature = 0.7

[mistral-7b]
model = /models/mistral-7b.gguf
temperature = 0.5
```

### File Location

Preset files are stored in: `./config/` directory

- `config/my-preset.ini`
- `config/production.ini`
- `config/testing.ini`

## Integration Points

### Frontend (presets.js page)

Add a "Launch Server" button to the presets UI:

```javascript
async _handleStartServer() {
  try {
    const response = await stateManager.request("presets:start-with-preset", {
      filename: this.state.selectedPreset.name,
      options: {
        maxModels: 4,
        threads: 4,
        ctxSize: 4096
      }
    });

    if (response.success) {
      showNotification(
        `Server started on port ${response.data.port}`,
        "success"
      );
    } else {
      showNotification(
        `Error: ${response.error.message}`,
        "error"
      );
    }
  } catch (error) {
    console.error("[PRESETS] Start error:", error);
    showNotification("Failed to start server", "error");
  }
}
```

### Command Line (Alternative)

You can still use presets manually from the command line:

```bash
# Using preset file
llama-server --models-preset ./config/my-preset.ini --models-max 4

# Preset includes all model configurations
# No need to specify --models-dir
```

## Benefits

1. **Complete Configuration**: All model settings encapsulated in preset file
2. **Consistency**: Same configuration for development and production
3. **Simplicity**: No CLI argument juggling
4. **Persistence**: Configuration stored and reusable
5. **Version Control**: Presets can be committed to git
6. **Easy Switching**: Switch between different configurations instantly

## File Paths

```
/home/bamer/nextjs-llama-async-proxy/
├── config/                          # Preset storage
│   ├── default.ini
│   ├── production.ini
│   └── my-custom-preset.ini
│
├── server/
│   └── handlers/
│       ├── presets.js               # Preset socket handlers (MODIFIED)
│       └── llama-router/
│           └── start.js             # Router startup (MODIFIED)
│
└── public/js/pages/
    └── presets.js                   # Frontend UI (can add launch button)
```

## Llama-Server Router Mode Reference

### Auto-Discovery Mode (existing)

```bash
llama-server \
  --port 8080 \
  --models-dir /path/to/models \
  --models-max 4
```

Loads all `.gguf` files from directory automatically.

### Preset Mode (new)

```bash
llama-server \
  --port 8080 \
  --models-preset /path/to/config.ini \
  --models-max 4
```

Loads models defined in `.ini` file with specific configurations.

## Testing

### Test Start with Preset

```bash
# Create a test preset
curl -X POST http://localhost:3000/presets \
  -H "Content-Type: application/json" \
  -d '{
    "filename": "test-preset",
    "config": {
      "*": { "ctx-size": 4096 },
      "test-model": { "model": "/path/to/test.gguf" }
    }
  }'

# Launch with preset
curl -X POST http://localhost:3000/presets/launch \
  -H "Content-Type: application/json" \
  -d '{
    "filename": "test-preset",
    "options": { "maxModels": 2 }
  }'
```

## Troubleshooting

### Preset File Not Found

**Error**: `Preset file not found: my-preset`

**Solution**: Ensure preset was created and saved to `./config/my-preset.ini`

```bash
ls -la ./config/
```

### llama-server Port In Use

**Error**: `Port already in use`

**Solution**: The system will automatically find the next available port. Check the response `data.port`.

### Model Path Not Found

**Error**: `Model file not found: /path/to/model.gguf`

**Solution**: Update preset model paths to valid absolute paths to `.gguf` files.

## Related Documentation

- [AGENTS.md](AGENTS.md) - Project guidelines
- [PRESETS_INTEGRATION_COMPLETE.md](PRESETS_INTEGRATION_COMPLETE.md) - Full preset system
- [INI_WEBSOCKET_INTEGRATION.md](INI_WEBSOCKET_INTEGRATION.md) - WebSocket integration

---

**Status**: ✅ Integration complete and ready for use
**Modified Files**: 
- `server/handlers/llama-router/start.js`
- `server/handlers/presets.js`
