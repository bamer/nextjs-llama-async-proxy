{
  "llama_options": {
    "basic": {
      "ctx_size": {
        "short": "-c",
        "long": "--ctx-size",
        "type": "number",
        "default": 4096,
        "min": 128,
        "max": 2000000,
        "description": "Size of the prompt context (0 = loaded from model)",
        "tooltip": "Maximum number of tokens in the context. Larger values allow longer conversations but use more VRAM."
      },
      "batch_size": {
        "short": "-b",
        "long": "--batch-size",
        "type": "number",
        "default": 2048,
        "min": 1,
        "description": "Logical maximum batch size",
        "tooltip": "Maximum tokens processed in a single batch. Higher values = faster but more VRAM."
      },
      "ubatch_size": {
        "short": "-ub",
        "long": "--ubatch-size",
        "type": "number",
        "default": 512,
        "min": 1,
        "description": "Physical maximum batch size",
        "tooltip": "Hardware-level batch size. Usually leave at default or half of batch_size."
      },
      "threads": {
        "short": "-t",
        "long": "--threads",
        "type": "number",
        "default": -1,
        "description": "Number of CPU threads to use during generation (-1 = auto)",
        "tooltip": "How many CPU cores to use. Set to number of physical cores for best performance."
      },
      "threads_batch": {
        "short": "-tb",
        "long": "--threads-batch",
        "type": "number",
        "default": -1,
        "description": "Number of threads for batch and prompt processing (default: same as --threads)",
        "tooltip": "Thread count specifically for batch processing. -1 = same as --threads."
      },
      "n_predict": {
        "short": "-n",
        "long": "--predict",
        "type": "number",
        "default": -1,
        "description": "Number of tokens to predict (-1 = infinity)",
        "tooltip": "Maximum tokens to generate per completion. -1 means unlimited."
      },
      "seed": {
        "short": "-s",
        "long": "--seed",
        "type": "number",
        "default": -1,
        "description": "RNG seed (-1 = random seed)",
        "tooltip": "Random number seed for reproducible results. Same seed = same output."
      },
      "port": {
        "short": "--port",
        "type": "number",
        "default": 8080,
        "min": 1024,
        "max": 65535,
        "description": "Port to listen on",
        "tooltip": "The port number for the HTTP server."
      },
      "host": {
        "short": "--host",
        "type": "string",
        "default": "127.0.0.1",
        "description": "IP address to listen on or UNIX socket path",
        "tooltip": "Bind address. Use 0.0.0.0 to listen on all interfaces."
      }
    },
    "gpu": {
      "gpu_layers": {
        "short": "-ngl",
        "long": "--gpu-layers",
        "type": "number",
        "default": -1,
        "description": "Max number of layers to store in VRAM (-1 = all)",
        "tooltip": "How many model layers to offload to GPU. -1 = all layers (if VRAM allows)."
      },
      "n_cpu_moe": {
        "short": "-ncmoe",
        "long": "--n-cpu-moe",
        "type": "number",
        "default": 0,
        "description": "Number of MoE layers to keep in CPU",
        "tooltip": "For Mixture of Experts models: number of first layers to keep on CPU, rest on GPU. This prevents GPU VRAM exhaustion."
      },
      "cpu_moe": {
        "short": "-cmoe",
        "long": "--cpu-moe",
        "type": "boolean",
        "default": false,
        "description": "Keep all MoE weights in CPU",
        "tooltip": "Force all Mixture of Experts layers to stay on CPU. Slower but uses less GPU VRAM."
      },
      "main_gpu": {
        "short": "-mg",
        "long": "--main-gpu",
        "type": "number",
        "default": 0,
        "description": "GPU index to use for main computations",
        "tooltip": "Which GPU to prioritize (0 = first GPU)."
      },
      "tensor_split": {
        "short": "-ts",
        "long": "--tensor-split",
        "type": "string",
        "description": "Tensor split proportions (e.g. 3,1 for 2 GPUs)",
        "tooltip": "Distribution of model layers across multiple GPUs. Example: '3,1' = 75% on GPU0, 25% on GPU1."
      },
      "split_mode": {
        "short": "-sm",
        "long": "--split-mode",
        "type": "select",
        "options": ["none", "layer", "row"],
        "default": "layer",
        "description": "How to split model across GPUs",
        "tooltip": "layer = split layers (default), row = split within layers for better GPU utilization."
      },
      "device": {
        "short": "-dev",
        "long": "--device",
        "type": "string",
        "description": "Comma-separated GPU device list (none = no offload)",
        "tooltip": "Specify which devices to use. Use --list-devices to see available."
      },
      "no_mmap": {
        "short": "",
        "long": "--no-mmap",
        "type": "boolean",
        "default": false,
        "description": "Disable memory mapping",
        "tooltip": "Don't use mmap for model loading. Slower load but may reduce system pageouts."
      },
      "mlock": {
        "short": "",
        "long": "--mlock",
        "type": "boolean",
        "default": false,
        "description": "Lock model in RAM (prevent swapping)",
        "tooltip": "Keep model in physical RAM, never swap to disk. Requires enough RAM."
      }
    },
    "performance": {
      "parallel": {
        "short": "-np",
        "long": "--parallel",
        "type": "number",
        "default": 1,
        "min": 1,
        "description": "Number of parallel sequences to decode",
        "tooltip": "Process multiple prompts simultaneously. Increases latency but throughput."
      },
      "flash_attn": {
        "short": "-fa",
        "long": "--flash-attn",
        "type": "select",
        "options": ["on", "off", "auto"],
        "default": "auto",
        "description": "Flash Attention optimization",
        "tooltip": "'auto' = use if supported by GPU, 'on' = force enable, 'off' = disable."
      },
      "kv_unified": {
        "short": "-kvu",
        "long": "--kv-unified",
        "type": "boolean",
        "default": false,
        "description": "Use unified KV buffer for all sequences",
        "tooltip": "Better memory efficiency for parallel decoding."
      },
      "cont_batching": {
        "short": "-cb",
        "long": "--cont-batching",
        "type": "boolean",
        "default": true,
        "description": "Enable continuous batching (dynamic batching)",
        "tooltip": "Improves GPU utilization by interleaving requests."
      },
      "cache_reuse": {
        "short": "",
        "long": "--cache-reuse",
        "type": "number",
        "default": 0,
        "description": "Min chunk size for KV cache reuse via shifting",
        "tooltip": "Enables KV cache reuse for similar prompts, speeds up repeated requests."
      },
      "no_warmup": {
        "short": "",
        "long": "--no-warmup",
        "type": "boolean",
        "default": false,
        "description": "Skip warming up the model",
        "tooltip": "Skip initial empty run. Faster startup but first generation slightly slower."
      }
    },
    "sampling": {
      "temperature": {
        "short": "--temp",
        "type": "number",
        "default": 0.8,
        "min": 0.0,
        "max": 2.0,
        "step": 0.1,
        "description": "Sampling temperature",
        "tooltip": "0 = deterministic (same output), higher = more creative/random, 2.0 = very random."
      },
      "top_k": {
        "short": "--top-k",
        "type": "number",
        "default": 40,
        "min": 0,
        "description": "Top-K sampling (0 = disabled)",
        "tooltip": "Only sample from top K most likely tokens. Lower = more focused."
      },
      "top_p": {
        "short": "--top-p",
        "type": "number",
        "default": 0.9,
        "min": 0.0,
        "max": 1.0,
        "step": 0.05,
        "description": "Top-P (nucleus) sampling",
        "tooltip": "Keep tokens with cumulative probability <= P. More natural than top-k."
      },
      "min_p": {
        "short": "--min-p",
        "type": "number",
        "default": 0.1,
        "min": 0.0,
        "max": 1.0,
        "step": 0.05,
        "description": "Min-P sampling (0 = disabled)",
        "tooltip": "Filter out tokens below min probability relative to best token."
      },
      "repeat_penalty": {
        "short": "--repeat-penalty",
        "type": "number",
        "default": 1.0,
        "min": 0.0,
        "max": 2.0,
        "step": 0.05,
        "description": "Penalize repeated tokens",
        "tooltip": "1.0 = no penalty, >1.0 = discourage repetition, <1.0 = encourage."
      },
      "repeat_last_n": {
        "short": "--repeat-last-n",
        "type": "number",
        "default": 64,
        "min": 0,
        "description": "Last N tokens to consider for penalty (-1 = ctx_size)",
        "tooltip": "How many recent tokens to check for repetition."
      },
      "mirostat": {
        "short": "--mirostat",
        "type": "select",
        "options": [0, 1, 2],
        "default": 0,
        "description": "Mirostat sampling (0=disabled, 1=v1, 2=v2.0)",
        "tooltip": "Maintains consistent entropy. Alternative to temperature-based sampling."
      },
      "mirostat_eta": {
        "short": "--mirostat-eta",
        "type": "number",
        "default": 0.1,
        "min": 0.0,
        "max": 1.0,
        "step": 0.05,
        "description": "Mirostat learning rate (eta)",
        "tooltip": "How quickly Mirostat adapts. Lower = slower adaptation."
      },
      "mirostat_tau": {
        "short": "--mirostat-tau",
        "type": "number",
        "default": 5.0,
        "min": 0.0,
        "max": 10.0,
        "step": 0.5,
        "description": "Mirostat target entropy (tau)",
        "tooltip": "Target entropy level. Higher = more diversity."
      }
    },
    "advanced": {
      "rope_scaling": {
        "short": "",
        "long": "--rope-scaling",
        "type": "select",
        "options": ["none", "linear", "yarn"],
        "description": "RoPE frequency scaling method",
        "tooltip": "How to handle context length beyond training. 'yarn' is most robust."
      },
      "rope_scale": {
        "short": "",
        "long": "--rope-scale",
        "type": "number",
        "default": 1.0,
        "min": 1.0,
        "max": 4.0,
        "step": 0.1,
        "description": "RoPE context scaling factor",
        "tooltip": "Multiply context length. 2.0 = double context (may reduce quality)."
      },
      "rope_freq_base": {
        "short": "",
        "long": "--rope-freq-base",
        "type": "number",
        "description": "RoPE base frequency (0 = from model)",
        "tooltip": "Advanced: frequency base for positional embeddings."
      },
      "no_kv_offload": {
        "short": "-nkvo",
        "long": "--no-kv-offload",
        "type": "boolean",
        "default": false,
        "description": "Disable KV offload",
        "tooltip": "Keep KV cache on GPU. Faster but uses more VRAM."
      },
      "cache_type_k": {
        "short": "-ctk",
        "long": "--cache-type-k",
        "type": "select",
        "options": ["f32", "f16", "bf16", "q8_0", "q4_0", "q4_1"],
        "default": "f16",
        "description": "KV cache data type for K",
        "tooltip": "Lower precision = less VRAM but may reduce quality. q8_0/q4_0 = quantized."
      },
      "cache_type_v": {
        "short": "-ctv",
        "long": "--cache-type-v",
        "type": "select",
        "options": ["f32", "f16", "bf16", "q8_0", "q4_0", "q4_1"],
        "default": "f16",
        "description": "KV cache data type for V",
        "tooltip": "Same as K. Both should typically match."
      },
      "numa": {
        "short": "",
        "long": "--numa",
        "type": "select",
        "options": ["distribute", "isolate", "numactl"],
        "description": "NUMA optimization strategy",
        "tooltip": "For multi-socket systems: 'distribute' spreads across nodes."
      }
    },
    "logging": {
      "verbose": {
        "short": "-v",
        "long": "--verbose",
        "type": "boolean",
        "default": false,
        "description": "Verbose logging (log all messages)",
        "tooltip": "Enable debug output. Useful for troubleshooting."
      },
      "log_colors": {
        "short": "",
        "long": "--log-colors",
        "type": "select",
        "options": ["on", "off", "auto"],
        "default": "auto",
        "description": "Colored logging output",
        "tooltip": "'auto' = color if terminal, 'on' = always, 'off' = never."
      },
      "log_timestamps": {
        "short": "",
        "long": "--log-timestamps",
        "type": "boolean",
        "default": false,
        "description": "Add timestamps to log messages",
        "tooltip": "Prefix each log line with timestamp."
      },
      "no_perf": {
        "short": "",
        "long": "--no-perf",
        "type": "boolean",
        "default": false,
        "description": "Disable performance timings",
        "tooltip": "Skip internal timing measurements for slight performance gain."
      }
    },
    "special": {
      "jinja": {
        "short": "",
        "long": "--jinja",
        "type": "boolean",
        "default": true,
        "description": "Use Jinja templates for chat",
        "tooltip": "Enable model's built-in chat template (most models use this)."
      },
      "metrics": {
        "short": "",
        "long": "--metrics",
        "type": "boolean",
        "default": false,
        "description": "Enable Prometheus metrics endpoint",
        "tooltip": "Expose /metrics endpoint for monitoring systems."
      },
      "no_webui": {
        "short": "",
        "long": "--no-webui",
        "type": "boolean",
        "default": false,
        "description": "Disable built-in web UI",
        "tooltip": "Disable llama-server's default web interface."
      },
      "embedding": {
        "short": "",
        "long": "--embedding",
        "type": "boolean",
        "default": false,
        "description": "Restrict to embedding use case only",
        "tooltip": "Use only with dedicated embedding models."
      },
      "timeout": {
        "short": "-to",
        "long": "--timeout",
        "type": "number",
        "default": 6000,
        "min": 1,
        "description": "Server read/write timeout in seconds",
        "tooltip": "How long to wait for client responses before timing out."
      }
    }
  }
}
