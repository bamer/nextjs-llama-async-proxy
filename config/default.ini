LLAMA_CONFIG_VERSION = 1

[*]
model = 
ctx-size = 131000
threads = 0
temp = 0.7
n-gpu-layers = 0
batch = 512
ubatch = 512

[qween groups]
model = 

[qween group tests]
model = 

